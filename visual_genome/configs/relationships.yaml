DATA_FOLDER: "/nobackup3/hkhader/datasets/olive"

vision_encoder: "openai/clip-vit-large-patch14-336"

llm_model: "gpt2"

no_compression: False
freeze_vision_encoder: True
freeze_llm: False


task: "relation_prediction"

# Use object level annotations for training/eval image captioning/VQA
use_object_annotations: True
system_prompt: "You are a helpful vision assistant trained to help people analyze images."
device: "cuda:0"

n_decoder_layers: 32
batch_size: 2
learning_rate: 0.00002
early_stopping: False
check_loss_steps: 3000
examples_per_class: 1000000
pretrained_object_encoder_checkpoint: "None"
n_epochs: 1


#retrieval_set_path: "retrieval/medical_object_classification/retrieval_set_1000000_cropped_clip-vit-large-patch14-336.pkl"
# my generated retrival set
retrieval_set_path: "/u/h/k/hkhader/research/datasets/outputs/olive/retrieval/relation_prediction/retrieval_set_1000000_clip-vit-large-patch14-336.pkl"

use_retrieval: False # Whether or not to retrieve in-context examples
retrieval_k: 5 # Number of retrieved in-context examples
majority_vote_retrieval: False
use_image_features: False # Concatenate ViT Image Features to LLM Input Embeddings (slow!)
crop_image: False


n_patches: 24



# model name which will be used to save it or load it to resume training. Othwewise it choose default starting at save_folder/default_name
load_model_path: "/u/h/k/hkhader/research/datasets/outputs/olive/checkpoints/gpt2_genome/random_split_bs2_400k"
# save folder if load_model_path not provided. 
save_folder: "/nobackup3/hkhader/datasets/outputs/olive/checkpoints"
# add prefix to the default name saving. 
model_save_load_name_prefix: "" # add it to the default naming


# starting model is this. We still use for saving load_model_path/defaults. 
load_pretrained_model: False
pretrained_path: "/u/h/k/hkhader/research/datasets/outputs/olive/checkpoints/gpt2_genome/test"

