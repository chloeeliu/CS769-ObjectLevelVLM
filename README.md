# cs769-vlm-project

Advancements in vision-language models (VLMs) have significantly impacted tasks like image captioning, object recognition, and multimodal reasoning. To further enhance the effectiveness, efficiency, and generalization capabilities of VLMs, OLIVE (Ossowski and Hu, 2024) introduces a groundbreaking approach by representing images through their constituent objects rather than traditional image patch grids. This innovative method encodes image objects as discrete tokens, which are processed by a fine-tuned large language model (LLM) alongside an object encoder, enabling a more structured and semantic representation.

By adopting this object-centric representation, OLIVE facilitates object-level retrieval from a repository, dramatically improving the model's ability to generalize across diverse scenarios. This approach has shown remarkable success in addressing challenging tasks, including classification and captioning of rare objects and tackling previously unseen domains.

Building on OLIVE's foundational framework, this project aims to extend its capabilities in several pivotal areas. These enhancements are discussed in detail in the subsequent sections, offering new insights into the potential of object-based representations for vision-language tasks.

![image](https://github.com/user-attachments/assets/05daef1f-bf49-4cf3-a343-e20878fff3b8)

![image](https://github.com/user-attachments/assets/860cfcce-3e18-48dc-985c-cb0fd0bde28d)

![image](https://github.com/user-attachments/assets/8ff339c3-e558-4fd5-a142-d5d54d3b8bae)

