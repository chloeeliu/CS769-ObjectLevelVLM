# SAM Integration

## Overview
This repository contains the code for integrating [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything) 
with [OLIVE](https://github.com/tossowski/Olive/tree/main).

## Notable changes
Instead of using the ground truth COCO segmentations, this code provides the config options to run evaluation
(on object classification task) using masks generated by SAM.

Multiple settings for integrating SAM with OLIVE are provided in the config, along with a script to visualize and 
compare SAM generated masks with the ground truths.

## Running with SAM
### Setup
Setup is identical to the original OLIVE repository, which recommends setting up an anaconda environment:
```
conda env create -f environment.yml
conda activate olive
```

The necessary datasets can then be downloaded and preprocessed using the same OLIVE setup script:
```
python setup/setup.py
```
Note that due to technical limitations, only the first 1/4 of the train split was downloaded and used for SAM experiments.

The next setup step is preparing the retrieval set, which again uses the same script as original OLIVE:
```
python retrieve.py --train --config <path_to_config_file>
```

### Model checkpoints

The pretrained OLIVE model can be downloaded from HuggingFace using `git lfs` (make sure to update the config file with the model path).

For the reported experiments, [OLIVE-G-Classification](https://huggingface.co/tossowski/OLIVE-G-Classification) was used for all settings.

The SAM checkpoint can be downloaded from the [repo](https://github.com/facebookresearch/segment-anything?tab=readme-ov-file#model-checkpoints). For the reported experiments, [ViT-L SAM model](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth) was used.

### Config files
The config files contain 5 new possible fields:
```
use_sam: <True if evaluating using SAM segmentations, False to use COCO ground truths>
sam_strategy: <"random" or "mean", for the corresponding segmentation strategy>
sam_multi: <True if using multimask generation, False to just take the top scoring mask>
sam_model_type: <"vit_l" is used for all experiments>
sam_checkpoint: <path to the SAM checkpoint (should match sam_model_type)>
```

An example config used for evaluating with SAM in the random setting with multimask generation enabled is provided in `configs/config_sam.yaml`.

### Evaluation
Finally, the OLIVE+SAM model can be evaluated by running:
```
python main.py --test --config <path_to_config_file>
```

The results will be stored in a `.pkl` file with the object classification results, and the object classification accuracy should be printed to the console.

### Analysis
Another script to compare the original OLIVE (with ground truths) results with the OLIVE+SAM results (for the mean point setting).

This can be run with:
```
python analyze_sam_results.py 
    --config <path_to_config_file> 
    --original_outputs <path_to_original_results_pkl> 
    --sam_outputs <path_to_sam_mean_results_pkl> 
    --k <number of examples to output>
```

This script will collect examples where the OLIVE+SAM results outperformed the original setting. 

These examples are visualized by displaying each mask over the original image. These images are saved to a `./analysis` folder in `.png` format.
